#Imported the necessary libraries for the programme
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.model_selection import GridSearchCV
import xgboost as xgb
import matplotlib.pyplot as plt

#Loaded the dataset available in csv format
data = pd.read_csv(r"C:\Users\Gupta\Downloads\btp_data.csv")

#Convert the categorical variables to numerical using one-hot encoding
data = pd.get_dummies(data, columns=['Compensating_Element', 'Recess_Shape'])
y = data["Load_Capacity"]
# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data.drop('Load_Capacity', axis=1), data['Load_Capacity'], test_size=0.2, random_state=42)
# Define the search space for optimal values
param_grid = {
    'n_estimators': [90,100,110],
    'max_depth': [None,10,11,12],
    'min_samples_split': [1,2,3]
}

print("Searching for right hyperparameters...")
# Define the objective function
def objective(params):
    model = GradientBoostingRegressor(**params)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    score = mean_squared_error(y_test, y_pred)
    return score
print("Objective function complete...")
# Create a GridSearchCV object and run the search
grid_search = GridSearchCV(
    estimator=GradientBoostingRegressor(random_state=42),
    param_grid=param_grid,
    scoring='neg_mean_squared_error',
    cv=5,
)
grid_search.fit(X_train, y_train)
print('Best parameters:', grid_search.best_params_)
print('Best score:', grid_search.best_score_)

#Random Forest Regression Model
n_estimators=best_params
random_state=best_score 
#best for n=100 and random_state=42

kfold = KFold(n_splits=5, shuffle=True)

# XGBoost Regressor
xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=n_estimators, random_state=random_state)
xgb_model.fit(X_train, y_train)

#Make predictions on the testing set
y_pred = xgb_model.predict(X_test)

#Evaluate the model's performance using mse and rsquared
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("XGBoost Regression Model: ")
print("min Mean Squared Error: ", mse)
print("max R-Squared: ", r2,"\n")
print(" n_estimators=",n_estimators," random_state= ",random_state)

#plotting the predicted data and the actual data on a graph.
plt.scatter(y_test, y_pred, alpha=0.5)
plt.plot([0, max(y_test)], [0, max(y_pred)], 'r--', label='Perfect Prediction')
plt.xlabel('Actual Load Capacity')
plt.ylabel('Predicted Load Capacity')
plt.title('XGBoost Regression (R^2 = {:.2f})'.format(r2))
plt.legend()
plt.show()

